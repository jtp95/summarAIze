{"1": "Quantum Federated Learning with Quantum Data\nMahdi Chehimi\nDepartment of Electrical and Computer Engineering\nVirginia Tech\nBlacksburg, VA 24060\nmahdic@vt.edu\nWalid Saad\nDepartment of Electrical and Computer Engineering\nVirginia Tech\nBlacksburg, VA 24060\nwalids@vt.edu\nAbstract\nQuantum machine learning (QML) has emerged as a promising \ufb01eld that leans\non the developments in quantum computing to explore large complex machine\nlearning problems. Recently, some purely quantum machine learning models were\nproposed such as quantum convolutional neural networks (QCNN) to perform\nclassi\ufb01cation on quantum data. However, all of the existing QML models rely on\ncentralized solutions that cannot scale well for large-scale and distributed quantum\nnetworks. Hence, it is apropos to consider more practical quantum federated\nlearning (QFL) solutions tailored towards emerging quantum network architectures.\nIndeed, developing QFL frameworks for quantum networks is critical given the\nfragile nature of computing qubits and the dif\ufb01culty of transferring them. On top\nof its practical momentousness, QFL allows for distributing quantum learning by\nleveraging existing wireless communication infrastructure. This paper proposes\nthe \ufb01rst fully quantum federated learning framework that can operate over quantum\ndata and, thus, share the learning of quantum circuit parameters in a decentralized\nmanner. First, given the lack of existing quantum federated datasets in the literature,\nthe proposed framework begins by generating the \ufb01rst quantum federated dataset,\nwith a hierarchical data format, for distributed quantum networks. Then, clients\nsharing QCNN models are fed with the quantum data to perform a classi\ufb01cation task.\nSubsequently, the server aggregates the learnable quantum circuit parameters from\nclients and performs federated averaging. Extensive experiments are conducted to\nevaluate and validate the effectiveness of the proposed QFL solution. This work is\nthe \ufb01rst to combine Google\u2019s TensorFlow Federated and TensorFlow Quantum in a\npractical implementation.\n1\nIntroduction\nRecent advances in quantum computing have revolutionized the way computations are done and\nresulted in quantum computers that solve complex large-scale problems in an extremely faster manner\ncompared to classical computers [1]. For instance, in 2019, Google claimed achieving quantum\nsupremacy [2], as their quantum computers were able to solve a problem, that would require 10,000\nyears of computations on a classical computer, within about 200 seconds using a quantum computer.\nMoreover, IBM is actively developing larger quantum computers [3, 4] with a clear roadmap for\nscaling up quantum technologies [4]. These major advances in quantum computing are rapidly\nPreprint. Under review.\narXiv:2106.00005v1  [quant-ph]  30 May 2021\n", "2": "leading to the deployment of quantum computers in a wide range of applications including \ufb01nance\n[5], communication networks, arti\ufb01cial intelligence (AI), and machine learning (ML) [6].\n1.1\nQuantum Machine Learning\nThe ability of quantum computers to handle exponential growth in the dimensions of data and perform\nlinear algebra faster and more ef\ufb01ciently than classical computers lead to the blossoming of the \ufb01eld of\nquantum machine learning (QML) [7]. For instance, various hybrid quantum-classical ML algorithms\nhave emerged recently, and, when run on quantum computers, achieved superior performance over\ntheir corresponding purely classical counterparts. These hybrid algorithms covered different areas in\nML such as supervised learning [8], quantum support vector machines [9], unsupervised learning\nsuch as clustering [10, 11], and quantum recommender systems [12].\nWith the recent developments of advanced quantum hardware and simulation systems, problems\nconcerned with quantum many-body systems [13] become of particular importance. However,\nthese problems have an extremely complex nature and exponentially large Hilbert spaces which\nrequires exponentially dif\ufb01cult quantum state tomography to translate them into a classical framework\nef\ufb01ciently. Moreover, the theoretical analysis of such complex, intrinsically quantum problems is\noften intractable. All these complex features impose major challenges (see [14]) on classical ML and\nhybrid QML models, rendering them inef\ufb01cient when addressing such challenging problems.\nThese challenges motivated the recent development of a number of purely quantum ML models that\ncan, in contrast to non-quantum or hybrid schemes, handle the complex nature of quantum many-body\nsystems. In particular, parametrized quantum circuits (PQC) [15] were proposed as the quantum\nversion of classical neural networks (NNs), forming what is known as quantum neural networks\n(QNN). QNNs are quantum circuits with tunable parameters that can be \u201clearned\u201d in a similar fashion\nto classical NNs.\nSeveral QNN architectures were developed in [16, 17] with the goal of performing classi\ufb01cation on\nnear-term quantum computers and training QNNs. Moreover, quantum convolutional neural networks\n(QCNNs) were proposed in [14] and achieved a promising performance on classi\ufb01cation tasks.\nVery recently, the authors in [18] demonstrated, using actual quantum computers, the advantages\nof QNNs compared with classical NNs. They veri\ufb01ed that, if designed effectively, QNNs achieve a\nhigher effective dimension from an information geometry point of view, in addition to faster training\ncapabilities compared to classical NNs. These \ufb01ndings highly motivate further investigation in the\narea of QNN given the great potential in this \ufb01eld.\n1.2\nQuantum Communication Networks\nAlong with the advances in QML, integrating quantum computers in future communication systems\nis necessary to handle the challenges caused by the rapidly growing volume of data. Moreover, the\nemerging networks of quantum sensors [19] and the envisioned quantum internet [20, 21] highly\nmotivate designing large-scale distributed quantum networks. Such networks would allow for\ndistributed quantum computing, large-scale quantum communications, and collaborations between\nquantum clients to perform common tasks.\nIn this context, existing quantum communication (QC) networks are particularly suitable for deploying\nsecure communications using the so-called quantum key distribution (QKD) and its variants [22].\nHowever, such networks usually rely on photonic quantum hardware that cannot perform strong QML\ncomputations, and, thus, they are only suitable for QKD encryption. In fact, the type of quantum\ncomputers needed to perform strong computations and run QML frameworks rely on advanced\nhardware and different technologies such as trapped-ions [23], quantum dots [24], superconducting\nqubits [25], among others. Such technologies require special conditions such as extremely low\ntemperatures, and vibration-free environments to store qubits and effectively maintain their quantum\nstates [1]. Thus, it is much more dif\ufb01cult to transfer the quantum data of QML models using QC\nchannels in an ef\ufb01cient manner. This is particularly true given the inherent problem of decoherence\nwhereby the quantum data sent in qubits decays gradually as it interacts with the environment, which\nshortens the lifetime of qubits [1].\nThus, there is a natural need for distributed learning solutions such as federated learning (FL) [26]\nin those emerging quantum networks. However, performing a distributed exchange of qubits is\n2\n", "3": "not effective for collaborative learning of purely quantum data since it requires the development of\nextremely complex advanced hardware which will not be available before a few decades [27]. Given\nthese inherent challenges of distributed quantum systems, one natural research question emerges: Can\nwe still perform collaborative quantum learning over quantum computing clients while leveraging\nexisting wireless technologies like 5G instead of relying on quantum channels?\nTowards answering this question, we observe that one can exploit the fact that FL algorithms allow\nthe exchange of model parameters, rather than the quantum data itself. As such, it is conceivable\nthat FL can be used to operate over quantum data and, then, leverage classical wireless channels to\nexchange the model parameters, without the need for a full QC network that is not available. However,\nremarkably, to the best of our knowledge, no prior work has proposed a thorough, comprehensive\nframework for implementing FL over purely quantum communication networks, as will be evident\nfrom Section 1.5. As such, this is the key problem we address here.\n1.3\nProblem Statement\nAs already noted, emerging large-scale QC networks will require collaborative learning between\nquantum clients. The qubits used for communication are fragile and susceptible for various losses\nwhich renders the ef\ufb01cient transmission of purely quantum data for QML applications a challenging\ntask. Moreover, all of the existing QML models rely on centralized solutions that cannot scale well\nfor future, large-scale and distributed quantum networks. Thus, there is a need for a comprehensive\nquantum federated learning (QFL) framework that allows for distributed quantum learning between\npurely quantum clients sharing a QML model without the need to transfer the quantum data itself.\nSuch a QFL framework should allow integrating quantum clients in existing classical wireless\ncommunication systems. Developing such a QFL framework requires answering many challenging\nquestions:\n\u2022 How can we generate quantum federated datasets, which are non-existent in prior art, in\norder to practically implement QFL?\n\u2022 Are existing classical FL algorithms capable of serializing and learning the quantum circuit\nparameters, or there is a need for new algorithms?\n\u2022 What are the practical challenges and constraints imposed by today\u2019s state-of-the-art quan-\ntum hardware on applying the proposed QFL framework?\n\u2022 Can the proposed QFL framework handle quantum data with different underlying distribu-\ntions (e.g., IID vs. non-IID) for different clients?\n1.4\nContributions\nThe main contribution of this paper is to address the aforementioned challenges by developing a\nnovel comprehensive federated learning framework that allows for distributing quantum learning\ncollaboratively between clients with purely quantum data (e.g., quantum sensors) while leveraging\nexisting classical wireless communication infrastructure. The effectiveness of the proposed approach\nis validated with extensive experiments and rigorous results that insightfully answer the fundamental\nchallenges posed in Section 1.3. The contributions of this work can be summarized as follows:\n\u2022 We propose the \ufb01rst purely quantum QFL framework for clients with quantum computing\ncapabilities that employ QCNN models to perform a classi\ufb01cation task by decentralizing the\nlearning of quantum circuit parameters and performing averaging on the server side.\n\u2022 The proposed QFL framework allows for integrating quantum computing clients with\nthe state-of-the-art infrastructure in classical communication networks without relying on\nquantum channels. Thus, the proposed framework is amenable to practical implementations\nin existing communication networks.\n\u2022 We generate a novel quantum federated dataset which can be used for distributed learning in\nQC networks. The generated dataset has a hierarchical data format and is the \ufb01rst of its kind\nin the literature. This dataset can serve as a baseline for future QFL implementations with\nquantum sensors.\n\u2022 We conduct the \ufb01rst extensive experiments that combine Google\u2019s TensorFlow Quantum\n(TFQ) [7] and TensorFlow Federated (TFF) [28]. The developed experiments verify the\n3\n", "4": "applicability and effectiveness of the proposed approach. The results show that the proposed\nQFL framework achieves comparable, and in some cases superior, performance compared\nto the centralized, purely QML setup. Our results also show that the model can handle IID\nand non-IID quantum data. A key \ufb01nding is that classical FL algorithms can be applied to\ndecentralize the learning in purely quantum QML applications.\n1.5\nRelated Works\nA handful of prior works [29, 30, 31, 32] exist on decentralized QML models, but those works\nneither address the previously posed challenges nor provide a solution to practically distribute\nquantum learning algorithms that are tailored towards emerging large-scale purely QC networks\nwith purely quantum QML. First, the authors in [29] proposed a protocol using which a single client\nthat does not have enough quantum capabilities performs a classical ML task (vector classi\ufb01cation)\nby communicating with a quantum server to run a small-scale QML model and send the learned\nparameters back to the client. The main objective of [29] is to preserve privacy of the communication\nlink while training the classical ML model. However, this work considers a single client, and it\ndoes not scale-up to emerging quantum networks that encompass multiple distributed clients as we\nconsider here. Also, this prior art [29] does not consider the deployment of quantum computing\nclients nor the integration of quantum data-based QML networks which is a challenging task that we\nwill address.\nMeanwhile, a concise vision of a QFL architecture for general optimization in wireless communication\nnetworks is discussed in [30]. This prior work considers a radically different scenario than the one\ntreated here. In particular, it considers a wireless network of classical non-quantum mobile users,\ncommunicating with access points that run a shallow QNN model for optimizing the wireless network.\nThe access points use FL and share the learning parameters with a server having another deeper\nQNN model. However, the work in [30] only discusses a conceptual architecture without any\nimplementation, veri\ufb01cation, or concrete results. Moreover, it does not consider quantum clients or\nquantum data in the communication network, and it relies solely on classical data.\nMeanwhile, the authors in [31] considered a vertical federated learning approach for decentralized\nfeature extraction in automatic speech recognition tasks. Their model includes a quantum server that\nuses a QCNN for feature extraction. However, this approach is totally different from our proposed\nframework as it primarily studies the usage of QCNNs for extracting useful features from classical\nnon-quantum data, and only assumes the server to have quantum capabilities in the communication\nnetwork.\nFinally, the most relevant prior work is the work in [32] which considered a hybrid quantum-classical\nML model trained in a federated setup. The work in [32] is different from our proposed QFL\nframework since it adopts a transfer learning approach where a pre-trained CNN model is used to\nextract features from classical data and compress it into a vector passed to a QNN to make predictions.\nAlthough the authors in [32] discuss federating the QML models, their implementation uses classical\ndata and has a very limited contribution to the purely quantum setup since it does not address the\nbiggest challenges such as the lack of a purely quantum federated dataset in the literature and the\nlack of a holistic implementation.\nClearly, there is no prior work that addresses QFL while leveraging purely quantum data learning.\nThis gap in the literature must be extensively addressed as it could provide a breakthrough in the way\nQC networks are looked at.\nThe rest of this paper is organized as follows. Section 2 describes the problem setup and the adopted\nQFL model in details. Next, the proposed quantum federated dataset is developed and the proposed\nprocess to generate it is presented in Section 3. In Section 4, we conduct the experiments and discuss\nthe key results. Conclusions are drawn in Section 6. Finally, insights on the challenges facing the\nproposed QFL framework and its broader impact are presented in Section 5.\n2\nProblem Setup\nWe consider a quantum network setup (Figure 1) consisting of a server and a set K of K quantum\ncomputing clients sharing a QCNN model such as the one proposed in [14]. Each client generates\nits own quantum data locally and trains a local QCNN model to perform binary classi\ufb01cation. The\n4\n", "5": "Figure 1: Problem Setup.\ngenerated data consists of excitations for an N-qubits quantum cluster state fed as labeled inputs in\npairs (|\u03c8m\u27e9, ym): m = 1, 2, ..., M, where |\u03c8m\u27e9represents the m-th sample input quantum state, ym\nis a binary label that classi\ufb01es whether the cluster state is excited (takes value of 0) or not (takes value\nof 1), and M is the number of data samples. Such a setup is typical for quantum sensor networks and\nwill be further analyzed in Section 3.\nIn an analogous manner to classical convolutional neural networks, and following a translationally\ninvariant behavior, the QCNN model includes a sequence of quantum convolution layers followed\nby quantum pooling layers. After incorporating suf\ufb01cient layers, a quantum fully connected layer is\napplied, and predictions are made by performing quantum measurement of qubits.\nMore formally, each convolution layer C \u2208Lc, with Lc being the set of convolution layers, is a single\nquasi-local unitary. This could represent any quantum gate, since unitarity is the main constraint that\nmust be imposed on any matrix that is used to represent a quantum gate [1]. In this regard, a given\nmatrix U corresponding to a single qubit gate is said to be unitary if U \u2020U = I, where U \u2020 is the\nadjoint of U, and I is the identity matrix. A unitary is called quasi-local if a quasi-local Hamiltonian\ngenerated that unitary as explained in [33].\nIn a quantum pooling layer P \u2208Lp, with Lp being the set of pooling layers, the system size (and the\ndegrees of freedom) are reduced, resulting in non-linearities. This is done by measuring some qubits\nand using the measurements to determine unitary rotations and apply them to other close qubits. After\nobtaining a system size that is suf\ufb01ciently small, a quantum fully connected layer F is applied on the\nremaining qubits and is represented by a unitary. Finally, measurement is performed on a speci\ufb01c\nnumber of qubits at the output to produce the QCNN\u2019s prediction.\nAt each client, the learnable parameters are the entries of all unitaries ,i.e., the quantum circuit\nparameters which are classical values. Let \u03b8k = (C, P, F) where k \u2208K be the vector of all QCNN\u2019s\nmodel parameters for client k, then the predicted output value by the QCNN model f to input quantum\nstate |\u03c8m\u27e9for client k is denoted by f\u03b8k(|\u03c8m\u27e9). Initially, the model parameters are initialized, and,\nthen, they are updated in order to minimize the following mean squared error (MSE) loss function:\narg min\n\u03b8k\nJ (\u03b8k) :=\n1\n2M\nM\nX\nm=1\n(ym \u2212f\u03b8k(|\u03c8m\u27e9))2.\n(1)\nIn order to bene\ufb01t from each others experience and data, all K clients will collaborate in training\nthe same QCNN model. Such a collaboration can be bene\ufb01cial for emerging applications such as\nlarge-scale quantum communication networks. To perform this collaboration, we use the proposed\nQFL framework. In this context, the general setup of QFL follows a similar structure to classical\nFL [34]. The collaborative learning is done by using existing wireless communication technologies\nsuch as the 5G cellular infrastructure to send the clients\u2019 \u201cclassical\u201d model parameters to the server.\n5\n", "6": "This setup will, in turn, allow the system to perform decentralized quantum data learning while using\nexisting classical wireless links, a task that is extremely dif\ufb01cult to achieve ef\ufb01ciently if the clients\nneeded to send the quantum data itself to the server.\nEach round h of the QFL training starts with the server sending its current version of the model\nparameters \u03b8s\nh to all K clients. Each client k \u2208K uses its local quantum data to run an optimization\nalgorithm such as stochastic gradient descent (SGD), in order to update its model parameters according\nto the following equation:\n\u03b8k\nh = \u03b8s\nh \u2212\u03b7 \u00b7 \u2207\u03b8k\nhJ (\u03b8k\nh),\n(2)\nfor every k \u2208K and with \u03b7 being the learning rate. Next, each client k sends its updated model\nparameters back to the server which aggregates the parameters from all clients. Then, the server\napplies the popular Federated Averaging1 FL algorithm [35] to estimate an average update of the\nmodel parameters and send the newly updated parameters to all clients according to the following\nrule:\n\u03b8s\nh+1 =\nX\nk\u2208K\nwk \u00b7 \u03b8k\nh,\n(3)\nwhere the weighting vector w = (w1, w2, ..., wk) is assigned by the server to weigh different clients\nin the network. This process is repeated until convergence.\nGiven this setup, our key goal is to implement the QFL framework over a quantum network in which\nquantum learning is performed in a decentralized manner using classical wireless communication\ninfrastructure. To do so, we next generate a novel quantum federated dataset, and, then, we implement\nthe QFL framework. This will constitute the \ufb01rst implementation of such a system that combines\nGoogle\u2019s TFQ and TFF.\n3\nQuantum Federated Dataset\nGiven the lack of existing quantum federated dataset in literature, our proposed QFL framework\nmust begin by generating the \ufb01rst quantum federated dataset that can be used for distributed learning\nin quantum networks. The generated dataset has a hierarchical data format and consists of purely\nquantum data. Next, we describe the proposed steps for generating the dataset.\n3.1\nQuantum Data\nA variety of purely quantum data exists for different quantum many-body systems [13]. The data is\ntypically obtained using different quantum devices or complex simulations of quantum systems.2\nFor instance, [14] considered symmetry-protected topological phases [37, 38] as input data to be\nclassi\ufb01ed by a QCNN model. We adopt the simpler, yet practical, form of quantum data consisting of\nquantum clusters inspired from [39].\nThe proposed quantum dataset consists of excitations of quantum cluster states [40, 41], labeled as\neither excited or not based on the rotations of the qubits. This type of data is of important use for\nquantum sensing networks like the ones applied for metrology [42]. In addition, quantum cluster\nstates are particularly important for our tackled practical problem of QC networks. This is due to their\napplicability in distributed quantum computing networks, and their ability to teleport quantum states\nbetween quantum clients communicating through a quantum channel [43]. Thus, the adopted data\ntype perfectly \ufb01ts our targets and allows for future extensions to quantum networks incorporating\nboth classical and quantum clients.\n3.2\nFederated Dataset Generation\n3.2.1\nGenerating Single Client Data\nWe used TFQ and Google\u2019s framework for quantum circuit programming: Cirq [44] for generating\nthe quantum data of each client. We begin by generating a rectangular grid of 1 \u00d7 8 qubits using\n1Other advanced FL algorithms can also be accomodated as part of our framework.\n2Note that classical data can be encoded into quantum states and fed to quantum computers as quantum data\nas discussed in [36] This classical data encoding task is done by performing quantum state preparation functions\nor by utilizing quantum feature maps.\n6\n", "7": "Cirq, such a size is reasonable for QML simulations and is suf\ufb01cient for validating the proposed\nQFL framework. Then, we create cluster states as TFQ circuits consisting of the Hadamard and\nControlled-Z quantum gates [1], and apply the circuit on the generated qubits. In order to de\ufb01ne the\nexcitations of cluster states, we observe that most quantum gates operating on a single qubit can be\ndescribed as rotations around an axis in the Bloch Sphere [45, 46]. Thus, they are usually referred\nto by their axis of rotation [44]. As proposed by [39], we consider excitations to be represented by\nrotation gates around the x-axis of the Bloch Sphere (Rx quantum gates [1]). An excitation of the\ncluster state is declared if a large enough rotation is achieved and the state is labeled with 1. In case\nthe rotation is not suf\ufb01ciently large, the state is declared as unexcited, and is labeled with a 0.\n3.2.2\nGenerating Federated Data\nThe previously described quantum data has input represented by quantum circuits. In order to be able\nto store the data, the quantum circuit is transformed into a tensor that is represented by strings. In\nfact, these strings represent an encoding of the serialized binary data of the quantum circuits with\nTensorFlow data type \u201clS5000\u201d. We particularly generate a hierarchical data format version 5 (HDF5)\nfederated dataset \ufb01le which includes examples of 30 clients (Any number of clients can be considered,\nsee Section 4). Each client has its own quantum dataset consisting of 160 serialized binary data input\nvector of a single feature, and a labels vector of integers of 0s and 1s.\nMoreover, we generate a quantum federated dataset consisting of non-IID clients\u2019 datasets. Then,\nwe conduct key experiments on this setup to compare the performance with the IID case originally\nfollowed. The obtained results are discussed in the next section.\n4\nExperiments\nIn this section, we present the experimental results for the proposed QFL framework. First, we\nprovide thorough details of the experimental setup. Then, we conduct extensive experiments to verify\nthe applicability and effectiveness of our proposed framework.\n4.1\nExperimental Setup\nImplementation. We use the TFQ and TFF frameworks to implement our proposed QFL framework,\nand we build upon the implementations in [39, 47]. We run our simulations on Google\u2019s Cloud\nPlatform known as \u201cGoogle Colaboratory [48]\u201d using CPU computing nodes. The implementation\nbegins with generating the quantum federated dataset described in Section 3.\nCompared to classical FL scenarios, it is natural to assume that the number of quantum computing\nclients in a quantum network will be in the range of tens of clients. It is typically assumed that,\nat a given point in time, only a subset of clients is available for training. However, for the ease of\nsimulations and since the number of clients is small, we assume that all quantum clients available for\ntraining, and we reserve the data of a small portion of clients for testing. Since TFF is currently only\navailable in simulation environments, then each client\u2019s data is assumed to be available locally.\nAs is typical in classical CNN models, the adopted QCNN architecture consists of quantum convo-\nlution layers, each followed by a quantum pooling layer. Then, a quantum fully connected layer is\nincluded, and \ufb01nally, the measurement is performed on the last layer. The width of the QCNN is not\nan optimization parameter since it solely relies on the number of qubits in the system, due to the\nreduction in size that quantum pooling layers introduce to the input qubits. In our case, and since we\nconsider 8 qubits in the QFL setup (this is a typical value for quantum sensing networks), we found\nthat having three pairs of quantum convolution-pooling layers, with 64 learnable parameters, is the\nmost suitable QCNN architecture that \ufb01ts our setup.\nOptimizers and hyperparameters. We compare the performance of the QFL framework under\nvarious client optimizers (Adam, SGD, and RMSprop) while \ufb01xing the server\u2019s optimizer to SGD\nwith a learning parameter equals to 1, since it is only performing averaging. The learning rates of the\nclients are varied, and their impact on the performance is discussed in Section 4.2.3.\nValidation metrics. While it is typical in centralized ML models to use validation data when training,\nthis sort of data is inaccessible in the QFL setup. Thus, a subset of clients will be speci\ufb01ed for\ntesting and validating the performance of the trained QFL framework. The validation metric used for\n7\n", "8": "6\n12\n18\n24\n30\nNumber of Clients\n94\n95\n96\n97\n98\n99\n100\nAccuracy (%)\nTrain\nTest\nCentralized Train\nCentralized Test\nFigure 2: Evaluation of QFL accuracy as the number of clients varies.\nthe testing clients is the binary accuracy metric with a threshold of 0.5, which is typical for binary\nclassi\ufb01cation problems.\n4.2\nResults and Discussion\n4.2.1\nImpact of Number of Clients\nWe begin our experiments by analyzing the impact of the number of available clients in the QFL\nnetwork. In Figure 2, we compare the achieved testing accuracy when the quantum federated dataset\nhas 1 (centralized), 6 (4 for training, 2 for testing), 12 (9 for training, 3 for testing), 18 (14 for training,\n4 for testing), 24 (19 for training, 5 for testing), and 30 (25 for training, 5 for testing) clients while\n\ufb01xing the number of data samples available to each client to 160 samples. We observe that, in general,\nas the number of participating clients in the QFL setup increases, a higher testing accuracy is achieved\nwithout over\ufb01tting the training data. The reason why the case of 6 clients achieves a smaller accuracy\nis because the number of clients in a federated setup must be large enough in order to achieve ef\ufb01cient\nlearning.\n4.2.2\nImpact of the Size of Clients\u2019 Data Samples\nIn Figure 3, we consider a QFL network of 30 active clients and compare the achieved testing\naccuracy with the centralized case (single client) while varying the size of the individual client\u2019s\ndatasets. Since the adopted QCNN is shallow and the number of trainable parameters is small (64),\nwe observe that increasing the size of the dataset does not necessarily guarantee an improvement in\nthe performance. In fact, as long as each client has enough data, increasing the size of the dataset may\nslightly increase or decrease the achieved testing accuracy. Another important observation is that, in\nthis network, the federated framework achieves a superior performance compared to the centralized\ncase. This is because each client in the federated setup bene\ufb01ts from the data of the other clients in\nthe learning process, which enhances the performance.\n4.2.3\nImpact of Optimizers and Learning Rates\nIn Figure 4, we compare the performance of the QFL framework with different optimizers and learning\nrates. We observe that the RMSprop optimizer with a learning rate of 0.002 is slow compared to the\nother optimizers and converges at a smaller testing accuracy. For the SGD optimizer with a learning\nrate of 0.02, we observe that it is the only one that achieves a high accuracy from the \ufb01rst epoch, and\nit converges to a value around 96.5%. However, the Adam optimizer with a learning rate of 0.02\nconverges to a higher testing accuracy after few training epochs. Finally, we observe that a learning\n8\n", "9": "80\n120\n160\n240\n320\nSize of Client's Dataset\n94\n95\n96\n97\n98\n99\n100\nTesting Accuracy (%)\nCentralized (1 client)\nFederated (30 clients)\nFigure 3: Size of client\u2019s dataset vs testing accuracy.\nrate of 0.2 for Adam optimizer is very large that it cannot learn, while a learning rate of 0.002 results\nin a smoother curve at the expense of a smaller accuracy.\n1\n2\n3\n4\n5\n6\n7\n8\n9\nEpochs\n50\n60\n70\n80\n90\n100\nTesting Accuracy (%)\nAdam (learning rate = 0.002)\nAdam (learning rate = 0.02)\nAdam (learning rate = 0.2)\nSGD (learning rate = 0.02)\nRMSprop (learning rate = 0.002)\nFigure 4: Evolution of the testing accuracy of different optimizers over the training epochs.\n4.2.4\nImpact of Non-IID Data\nWhen generating the quantum cluster states from the input qubits, the rotation values fed to the Rx\ngate were drawn from a uniform distribution between \u2212\u03c0 and \u03c0. In order to vary the underlying\ndistribution of the clients\u2019 data, we consider generating the data for half of the clients using a truncated\nnormal distribution, so that we generate non-IID quantum data. After generating the new dataset,\nwe compare the performance of the QFL framework on the IID and non-IID federated datasets for a\nnetwork of 30 clients (25 for training, 5 for testing) with 160 data samples each. In Table 1, we show\nthe testing accuracy and MSE loss for both datasets and observe that the proposed QFL framework\nachieves a similar performance on both IID and non-IID quantum federated datasets.\n9\n", "10": "Table 1: Performance comparison between IID and non-IID data\nDataset\nTesting Accuracy\nTesting MSE\nIID Data\n99.25\n5.66\nNon-IID Data\n98.625\n6.57\n4.2.5\nError Bars\nIn Figure 5, we show the error bars for the different performance metrics for both training and testing\ncases. The tested network consists of 30 clients each with a dataset of 160 samples and the data of 5\nclients is used for testing. Figure 5 shows the errors caused by the random seed after running multiple\nexperiments for the same network, and we can see that the errors are below 5% which is acceptable.\nTrain Accuracy\nTest Accuracy\nTrain MSE\nTest MSE\nPerformance Metrics\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\nPercentage Error (%)\nFigure 5: Error bars from the random seed after running experiments multiple times.\n5\nChallenges and Future Outlook\nThe state of the art near-term-intermediate-scale quantum (NISQ) hardware only allow for processing\na small number of qubits because of the dif\ufb01culty in performing quantum error correction to minimize\nthe losses. This is a challenge that renders the practical deployment of large-scale purely quantum\nQML models, like our proposed QFL framework, a dif\ufb01cult task. Also, the variety of technologies\nand the different quantum capabilities of devices in a quantum network may make the adoption of a\nuni\ufb01ed QML model dif\ufb01cult, and, thus, the practical implementation of our proposed QFL framework\nchallenging.\nHowever, our proposed QFL framework paves the way for integrating quantum devices with quantum\ndata with existing wireless networks. As a result, we anticipate that this can lead to a blossoming of\nnew applications that are coupled with new research problems in the areas of networking, quantum\nhardware, and wireless sensing. This includes designing ef\ufb01cient optimization algorithms that\naccount for networks including both quantum and classical computing clients. This is a great\nbreakthrough that allows leveraging the powerful computing capabilities of quantum computers\nin today\u2019s communication networks. Our proposed QFL framework allows for training the QML\nmodels inside future 6G communication networks which is very promising. Finally, in order to add\na new layer of security to our porposed QFL framework, one can investigate the use of quantum\ncryptographic schemes to encrypt the classical learning parameters in the QFL setup before sending\nthem to the server, and vice versa. Since the clients have quantum capabilities, integrating QFL with\nQKD is an inteteresting challenging problem that is worth investigation in the future.\n10\n", "11": "6\nConclusion\nIn this paper, we have proposed a novel framework for quantum federated learning that allows\nimplementing scalable distributed quantum learning over quantum data without the need to send\nqubits, but by leveraging classical wireless networks. To implement this framework, we have\ngenerated the \ufb01rst quantum federated dataset in the literature and performed a unique implementation\nthat combines TensorFlow Quantum and TensorFlow Federated. We have conducted extensive\nexperiments to verify the applicability and effectiveness of our proposed framework. The experimental\nresults validate the effective behavior of the proposed QFL framework using the federated averaging\nalgorithm.\nReferences\n[1] M. A. Nielsen and I. L. Chuang, Quantum Computation and Quantum Information: 10th\nAnniversary Edition.\nCambridge University Press, 2010.\n[2] F. Arute et al., \u201cQuantum supremacy using a programmable superconducting processor,\u201d\nNature, vol. 574, no. 7779, pp. 505\u2013510, Oct 2019. [Online]. Available:\nhttps:\n//doi.org/10.1038/s41586-019-1666-5\n[3] \u201cIbm delivers its highest quantum volume to date,\nexpanding the computational\npower of its ibm cloud-accessible quantum computers,\u201d Aug. 2020. [Online]. Available:\nhttps://newsroom.ibm.com/2020-08-20-IBM-Delivers-Its-Highest-Quantum-Volume-to-Dat\ne-Expanding-the-Computational-Power-of-its-IBM-Cloud-Accessible-Quantum-Computers\n[4] E. Pednault, J. A. Gunnels, G. Nannicini, L. Horesh, and R. Wisnieff, \u201cLeveraging secondary\nstorage to simulate deep 54-qubit sycamore circuits,\u201d 2019.\n[5] R. Orus, S. Mugel, and E. Lizaso, \u201cQuantum computing for \ufb01nance: Overview and prospects,\u201d\nReviews in Physics, vol. 4, p. 100028, 2019.\n[6] J. Preskill, \u201cQuantum computing in the nisq era and beyond,\u201d Quantum, vol. 2, p. 79, 2018.\n[7] M. Broughton, G. Verdon, T. McCourt, A. J. Martinez, J. H. Yoo, S. V. Isakov, P. Massey, M. Y.\nNiu, R. Halavati, E. Peters, M. Leib, A. Skolik, M. Streif, D. V. Dollen, J. R. McClean, S. Boixo,\nD. Bacon, A. K. Ho, H. Neven, and M. Mohseni, \u201cTensor\ufb02ow quantum: A software framework\nfor quantum machine learning,\u201d 2020.\n[8] M. Schuld, Supervised learning with quantum computers.\nSpringer, 2018.\n[9] P. Rebentrost, M. Mohseni, and S. Lloyd, \u201cQuantum support vector machine for big\ndata classi\ufb01cation,\u201d Phys. Rev. Lett., vol. 113, p. 130503, Sep 2014. [Online]. Available:\nhttps://link.aps.org/doi/10.1103/PhysRevLett.113.130503\n[10] S. Lloyd, M. Mohseni, and P. Rebentrost, \u201cQuantum algorithms for supervised and unsupervised\nmachine learning,\u201d 2013.\n[11] I. Kerenidis, J. Landman, A. Luongo, and A. Prakash, \u201cq-means: A quantum algorithm for\nunsupervised machine learning,\u201d in Advances in Neural Information Processing Systems 32,\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00e9-Buc, E. Fox, and R. Garnett, Eds.\nCurran Associates, Inc., 2019, pp. 4134\u20134144.\n[12] I. Kerenidis and A. Prakash, \u201cQuantum recommendation systems,\u201d 2016.\n[13] H. Tasaki, Physics and mathematics of quantum many-body systems.\nSpringer, 2020.\n[14] I. Cong,\nS. Choi,\nand M. D. Lukin,\n\u201cQuantum convolutional neural networks,\u201d\nNature Physics,\nvol. 15,\nno. 12,\npp. 1273\u20131278,\nDec 2019. [Online]. Available:\nhttps://doi.org/10.1038/s41567-019-0648-8\n[15] M. Benedetti, E. Lloyd, S. Sack, and M. Fiorentini, \u201cParameterized quantum circuits as\nmachine learning models,\u201d Quantum Science and Technology, vol. 4, no. 4, p. 043001, Nov\n2019. [Online]. Available: http://dx.doi.org/10.1088/2058-9565/ab4eb5\n[16] E. Farhi and H. Neven, \u201cClassi\ufb01cation with quantum neural networks on near term processors,\u201d\n2018.\n[17] K. Beer, D. Bondarenko, T. Farrelly, T. J. Osborne, R. Salzmann, D. Scheiermann, and R. Wolf,\n\u201cTraining deep quantum neural networks,\u201d Nature Communications, vol. 11, no. 1, p. 808, Feb\n2020. [Online]. Available: https://doi.org/10.1038/s41467-020-14454-2\n11\n", "12": "[18] A. Abbas, D. Sutter, C. Zoufal, A. Lucchi, A. Figalli, and S. Woerner, \u201cThe power of quantum\nneural networks,\u201d 2020.\n[19] X. Guo, C. R. Breum, J. Borregaard, S. Izumi, M. V. Larsen, T. Gehring, M. Christandl, J. S.\nNeergaard-Nielsen, and U. L. Andersen, \u201cDistributed quantum sensing in a continuous-variable\nentangled network,\u201d Nature Physics, vol. 16, no. 3, pp. 281\u2013284, 2020.\n[20] S. Pirandola and S. L. Braunstein, \u201cPhysics: Unite to build a quantum internet,\u201d Nature News,\nvol. 532, no. 7598, p. 169, 2016.\n[21] M. Calef\ufb01, A. S. Cacciapuoti, and G. Bianchi, \u201cQuantum internet: From communication to\ndistributed computing!\u201d in Proceedings of the 5th ACM International Conference on Nanoscale\nComputing and Communication, 2018, pp. 1\u20134.\n[22] C. H. Bennett and G. Brassard, \u201cQuantum cryptography: Public key distribution and coin\ntossing,\u201d Theoretical Computer Science, vol. 560, p. 7\u201311, Dec 2014. [Online]. Available:\nhttp://dx.doi.org/10.1016/j.tcs.2014.05.025\n[23] J. I. Cirac and P. Zoller, \u201cQuantum computations with cold trapped ions,\u201d Physical review letters,\nvol. 74, no. 20, p. 4091, 1995.\n[24] D. Loss and D. P. DiVincenzo, \u201cQuantum computation with quantum dots,\u201d Physical Review A,\nvol. 57, no. 1, p. 120, 1998.\n[25] G. Wendin, \u201cQuantum information processing with superconducting circuits: a review,\u201d Reports\non Progress in Physics, vol. 80, no. 10, p. 106001, 2017.\n[26] P. Kairouz et al., \u201cAdvances and open problems in federated learning,\u201d 2019.\n[27] M. Dyakonov, \u201cWhen will useful quantum computers be constructed? not in the foreseeable fu-\nture, this physicist argues. here\u2019s why: The case against: Quantum computing,\u201d IEEE Spectrum,\nvol. 56, no. 3, pp. 24\u201329, 2019.\n[28] A. Ingerman and K. Ostrowski, \u201cIntroducing tensor\ufb02ow federated,\u201d 2019. [Online]. Available:\nhttps://medium.com/tensorflow/introducing-tensor\ufb02ow-federated-a4147aa20041\n[29] Y.-B. Sheng and L. Zhou, \u201cDistributed secure quantum machine learning,\u201d Science Bulletin,\nvol. 62, no. 14, pp. 1025\u20131029, 06 2017.\n[30] B. Narottama and S. Shin, \u201cQuantum federated learning for wireless communications,\u201d in\nJournal of the Korean Institute of Communication Sciences, 2020.\n[31] C.-H. H. Yang, J. Qi, S. Y.-C. Chen, P.-Y. Chen, S. M. Siniscalchi, X. Ma, and C.-H. Lee,\n\u201cDecentralizing feature extraction with quantum convolutional neural network for automatic\nspeech recognition,\u201d 2020.\n[32] S. Y.-C. Chen and S. Yoo, \u201cFederated quantum machine learning,\u201d 2021.\n[33] M. Mari\u00ebn, K. M. R. Audenaert, K. V. Acoleyen, and F. Verstraete, \u201cEntanglement rates and the\nstability of the area law for the entanglement entropy,\u201d 2014.\n[34] F. X. Yu, A. S. Rawat, A. K. Menon, and S. Kumar, \u201cFederated learning with only positive\nlabels,\u201d 2020.\n[35] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, \u201cCommunication-\nef\ufb01cient learning of deep networks from decentralized data,\u201d 2017.\n[36] M. Schuld, R. Sweke, and J. J. Meyer, \u201cThe effect of data encoding on the expressive power of\nvariational quantum machine learning models,\u201d 2020.\n[37] Z.-C. Gu and X.-G. Wen, \u201cTensor-entanglement-\ufb01ltering renormalization approach and\nsymmetry-protected topological order,\u201d Physical Review B, vol. 80, no. 15, Oct 2009. [Online].\nAvailable: http://dx.doi.org/10.1103/PhysRevB.80.155131\n[38] N. Schuch, D. P\u00e9rez-Garc\u00eda, and I. Cirac, \u201cClassifying quantum phases using matrix product\nstates and projected entangled pair states,\u201d Physical Review B, vol. 84, no. 16, Oct 2011.\n[Online]. Available: http://dx.doi.org/10.1103/PhysRevB.84.165139\n[39] \u201cQuantum convolutional neural network: Tensor\ufb02ow quantum,\u201d 2020. [Online]. Available:\nhttps://www.tensor\ufb02ow.org/quantum/tutorials/qcnn\n[40] M. A. Nielsen, \u201cCluster-state quantum computation,\u201d Reports on Mathematical Physics, vol. 57,\nno. 1, p. 147\u2013161, Feb 2006. [Online]. Available: http://dx.doi.org/10.1016/S0034-4877(06)8\n0014-5\n12\n", "13": "[41] H. J. Briegel, Cluster States.\nBerlin, Heidelberg: Springer Berlin Heidelberg, 2009, pp.\n96\u2013105. [Online]. Available: https://doi.org/10.1007/978-3-540-70626-7_30\n[42] N. Friis, D. Orsucci, M. Skotiniotis, P. Sekatski, V. Dunjko, H. J. Briegel, and W. D\u00fcr, \u201cFlexible\nresources for quantum metrology,\u201d New Journal of Physics, vol. 19, no. 6, p. 063044, jun 2017.\n[Online]. Available: https://doi.org/10.1088/1367-2630/aa7144\n[43] I. B. Djordjevic, \u201cOn global quantum communication networking,\u201d Entropy, vol. 22, no. 8,\n2020. [Online]. Available: https://www.mdpi.com/1099-4300/22/8/831\n[44] Q. A. team and collaborators, \u201cCirq,\u201d Oct 2020. [Online]. Available: https://doi.org/10.5281/ze\nnodo.4062499\n[45] T. Lu, X. Miao, and H. Metcalf, \u201cBloch theorem on the bloch sphere,\u201d Physical Review A,\nvol. 71, no. 6, p. 061405, 2005.\n[46] I. Glendinning, \u201cThe bloch sphere,\u201d in QIA Meeting. Vienna, 2005.\n[47] \u201cFederated learning workshop using tensor\ufb02ow federated,\u201d July 2020. [Online]. Available:\nhttps://colab.research.google.com/drive/1kCSSFUCU_rxW7MElwZXENe50_VTnvGH1?usp\n=sharing\n[48] E. Bisong, Google Colaboratory.\nBerkeley, CA: Apress, 2019, pp. 59\u201364.\n13\n"}